{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bff6afb",
   "metadata": {},
   "source": [
    "# INSTALLING PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc73beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce483c",
   "metadata": {},
   "source": [
    "# READING DATA OF customerData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a2168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------------------+--------------+\n",
      "|cust_id|cust_fname|cust_lname|cust_order_quantity|payment_status|\n",
      "+-------+----------+----------+-------------------+--------------+\n",
      "|      1|     Rahul|     Kumar|                  5|          paid|\n",
      "|      2|     Priya|    Sharma|                  8|          paid|\n",
      "|      3|     Ankit|     Singh|                  3|        unpaid|\n",
      "|      4|     Sneha|     Verma|                  6|          paid|\n",
      "|      5|      Amit|     Joshi|                 10|        unpaid|\n",
      "|      6|      Neha|     Patel|                  4|          paid|\n",
      "|      7|       Raj|      Shah|                  7|        unpaid|\n",
      "|      8|    Deepak|     Reddy|                  2|          paid|\n",
      "|      9|     Pooja|    Chopra|                  9|          paid|\n",
      "|     10|    Suresh|    Mishra|                  5|        unpaid|\n",
      "|     11|     Aarti|     Verma|                  7|          paid|\n",
      "|     12|     Karan|     Mehta|                  3|        unpaid|\n",
      "|     13|    Anjali|   Agarwal|                  6|          paid|\n",
      "|     14|    Vikram|     Singh|                  8|        unpaid|\n",
      "|     15|    Shweta|     Gupta|                  4|          paid|\n",
      "|     16|     Rohit|    Kapoor|                 10|        unpaid|\n",
      "|     17|     Divya|    Mittal|                  5|          paid|\n",
      "|     18|      Ajay|     Kumar|                  7|        unpaid|\n",
      "|     19|      Mona|     Singh|                  2|          paid|\n",
      "|     20|     Arjun|    Sharma|                  9|          paid|\n",
      "+-------+----------+----------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CustomerData\").getOrCreate()\n",
    "\n",
    "# # Defining the path to the CSV file\n",
    "# csv_file_path = \"C:/Users/vaibhav/Desktop/jupyter projects/customerData.csv\"\n",
    "\n",
    "# # Reading CSV into a Spark DataFrame\n",
    "# customer_data = spark.read.csv(csv_file_path, header=True)\n",
    "\n",
    "# # Show the DataFrame\n",
    "# customer_data.show()\n",
    "\n",
    "# # Stop the Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9352592",
   "metadata": {},
   "source": [
    "# Concatenating fname & lname column to fullName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd7795ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------------------+--------------+--------------+\n",
      "|cust_id|cust_fname|cust_lname|cust_order_quantity|payment_status|cust_fullName |\n",
      "+-------+----------+----------+-------------------+--------------+--------------+\n",
      "|1      |Rahul     |Kumar     |5                  |paid          |Rahul Kumar   |\n",
      "|2      |Priya     |Sharma    |8                  |paid          |Priya Sharma  |\n",
      "|3      |Ankit     |Singh     |3                  |unpaid        |Ankit Singh   |\n",
      "|4      |Sneha     |Verma     |6                  |paid          |Sneha Verma   |\n",
      "|5      |Amit      |Joshi     |10                 |unpaid        |Amit Joshi    |\n",
      "|6      |Neha      |Patel     |4                  |paid          |Neha Patel    |\n",
      "|7      |Raj       |Shah      |7                  |unpaid        |Raj Shah      |\n",
      "|8      |Deepak    |Reddy     |2                  |paid          |Deepak Reddy  |\n",
      "|9      |Pooja     |Chopra    |9                  |paid          |Pooja Chopra  |\n",
      "|10     |Suresh    |Mishra    |5                  |unpaid        |Suresh Mishra |\n",
      "|11     |Aarti     |Verma     |7                  |paid          |Aarti Verma   |\n",
      "|12     |Karan     |Mehta     |3                  |unpaid        |Karan Mehta   |\n",
      "|13     |Anjali    |Agarwal   |6                  |paid          |Anjali Agarwal|\n",
      "|14     |Vikram    |Singh     |8                  |unpaid        |Vikram Singh  |\n",
      "|15     |Shweta    |Gupta     |4                  |paid          |Shweta Gupta  |\n",
      "|16     |Rohit     |Kapoor    |10                 |unpaid        |Rohit Kapoor  |\n",
      "|17     |Divya     |Mittal    |5                  |paid          |Divya Mittal  |\n",
      "|18     |Ajay      |Kumar     |7                  |unpaid        |Ajay Kumar    |\n",
      "|19     |Mona      |Singh     |2                  |paid          |Mona Singh    |\n",
      "|20     |Arjun     |Sharma    |9                  |paid          |Arjun Sharma  |\n",
      "+-------+----------+----------+-------------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import concat_ws\n",
    "\n",
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CustomerData\").getOrCreate()\n",
    "\n",
    "# # Define the path to the CSV file\n",
    "# csv_file_path = \"C:/Users/vaibhav/Desktop/jupyter projects/customerData.csv\"\n",
    "\n",
    "# # Read CSV into a Spark DataFrame\n",
    "# customer_data = spark.read.csv(csv_file_path, header=True)\n",
    "\n",
    "# # Concatenate cust_fname and cust_lname columns\n",
    "# customer_data = customer_data.withColumn(\"cust_fullName\", concat_ws(\" \", \"cust_fname\", \"cust_lname\"))\n",
    "\n",
    "# # Show the DataFrame with the updated column\n",
    "# customer_data.show(truncate=False)\n",
    "\n",
    "# # Stop the Spark session\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a1b8a",
   "metadata": {},
   "source": [
    "# Adding New Column Cart_Value in our table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b99e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------------------+--------------+--------------+----------+\n",
      "|cust_id|cust_fname|cust_lname|cust_order_quantity|payment_status|cust_fullName |cart_value|\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+\n",
      "|1      |Rahul     |Kumar     |5                  |paid          |Rahul Kumar   |4000      |\n",
      "|2      |Priya     |Sharma    |8                  |paid          |Priya Sharma  |3000      |\n",
      "|3      |Ankit     |Singh     |3                  |unpaid        |Ankit Singh   |4500      |\n",
      "|4      |Sneha     |Verma     |6                  |paid          |Sneha Verma   |5000      |\n",
      "|5      |Amit      |Joshi     |10                 |unpaid        |Amit Joshi    |4000      |\n",
      "|6      |Neha      |Patel     |4                  |paid          |Neha Patel    |3500      |\n",
      "|7      |Raj       |Shah      |7                  |unpaid        |Raj Shah      |1500      |\n",
      "|8      |Deepak    |Reddy     |2                  |paid          |Deepak Reddy  |3500      |\n",
      "|9      |Pooja     |Chopra    |9                  |paid          |Pooja Chopra  |3000      |\n",
      "|10     |Suresh    |Mishra    |5                  |unpaid        |Suresh Mishra |5000      |\n",
      "|11     |Aarti     |Verma     |7                  |paid          |Aarti Verma   |5000      |\n",
      "|12     |Karan     |Mehta     |3                  |unpaid        |Karan Mehta   |1500      |\n",
      "|13     |Anjali    |Agarwal   |6                  |paid          |Anjali Agarwal|4000      |\n",
      "|14     |Vikram    |Singh     |8                  |unpaid        |Vikram Singh  |500       |\n",
      "|15     |Shweta    |Gupta     |4                  |paid          |Shweta Gupta  |4500      |\n",
      "|16     |Rohit     |Kapoor    |10                 |unpaid        |Rohit Kapoor  |3000      |\n",
      "|17     |Divya     |Mittal    |5                  |paid          |Divya Mittal  |1000      |\n",
      "|18     |Ajay      |Kumar     |7                  |unpaid        |Ajay Kumar    |5000      |\n",
      "|19     |Mona      |Singh     |2                  |paid          |Mona Singh    |3500      |\n",
      "|20     |Arjun     |Sharma    |9                  |paid          |Arjun Sharma  |4500      |\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import concat_ws, rand, col, floor\n",
    "\n",
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder.appName(\"CustomerData\").getOrCreate()\n",
    "\n",
    "# # Define the path to the CSV file\n",
    "# csv_file_path = \"C:/Users/vaibhav/Desktop/jupyter projects/customerData.csv\"\n",
    "\n",
    "# # Read CSV into a Spark DataFrame\n",
    "# customer_data = spark.read.csv(csv_file_path, header=True)\n",
    "\n",
    "# # Concatenate cust_fname and cust_lname columns\n",
    "# customer_data = customer_data.withColumn(\"cust_fullName\", concat_ws(\" \", \"cust_fname\", \"cust_lname\"))\n",
    "\n",
    "# # Add a new column cart_value\n",
    "# customer_data = customer_data.withColumn(\"cart_value\", (floor(rand() * 10) + 1) * 500)\n",
    "\n",
    "# # Show the DataFrame with the updated columns\n",
    "# customer_data.show(truncate=False)\n",
    "\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111d7c0",
   "metadata": {},
   "source": [
    "# Adding a new column Age in our csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c85cb24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "|cust_id|cust_fname|cust_lname|cust_order_quantity|payment_status|cust_fullName |cart_value|Age|\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "|1      |Rahul     |Kumar     |5                  |paid          |Rahul Kumar   |3000      |38 |\n",
      "|2      |Priya     |Sharma    |8                  |paid          |Priya Sharma  |2500      |46 |\n",
      "|3      |Ankit     |Singh     |3                  |unpaid        |Ankit Singh   |1000      |27 |\n",
      "|4      |Sneha     |Verma     |6                  |paid          |Sneha Verma   |3500      |33 |\n",
      "|5      |Amit      |Joshi     |10                 |unpaid        |Amit Joshi    |2500      |42 |\n",
      "|6      |Neha      |Patel     |4                  |paid          |Neha Patel    |1000      |48 |\n",
      "|7      |Raj       |Shah      |7                  |unpaid        |Raj Shah      |4500      |40 |\n",
      "|8      |Deepak    |Reddy     |2                  |paid          |Deepak Reddy  |4000      |28 |\n",
      "|9      |Pooja     |Chopra    |9                  |paid          |Pooja Chopra  |5000      |48 |\n",
      "|10     |Suresh    |Mishra    |5                  |unpaid        |Suresh Mishra |500       |39 |\n",
      "|11     |Aarti     |Verma     |7                  |paid          |Aarti Verma   |500       |28 |\n",
      "|12     |Karan     |Mehta     |3                  |unpaid        |Karan Mehta   |2500      |30 |\n",
      "|13     |Anjali    |Agarwal   |6                  |paid          |Anjali Agarwal|500       |48 |\n",
      "|14     |Vikram    |Singh     |8                  |unpaid        |Vikram Singh  |5000      |29 |\n",
      "|15     |Shweta    |Gupta     |4                  |paid          |Shweta Gupta  |4500      |36 |\n",
      "|16     |Rohit     |Kapoor    |10                 |unpaid        |Rohit Kapoor  |4500      |47 |\n",
      "|17     |Divya     |Mittal    |5                  |paid          |Divya Mittal  |1000      |30 |\n",
      "|18     |Ajay      |Kumar     |7                  |unpaid        |Ajay Kumar    |1500      |34 |\n",
      "|19     |Mona      |Singh     |2                  |paid          |Mona Singh    |3000      |45 |\n",
      "|20     |Arjun     |Sharma    |9                  |paid          |Arjun Sharma  |1500      |29 |\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat_ws, rand, floor\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CustomerData\").getOrCreate()\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_file_path = \"C:/Users/vaibhav/Desktop/jupyter projects/customerData.csv\"\n",
    "\n",
    "# Read CSV into a Spark DataFrame\n",
    "customer_data = spark.read.csv(csv_file_path, header=True)\n",
    "\n",
    "# Concatenate cust_fname and cust_lname columns\n",
    "customer_data = customer_data.withColumn(\"cust_fullName\", concat_ws(\" \", \"cust_fname\", \"cust_lname\"))\n",
    "\n",
    "# Add a new column cart_value\n",
    "customer_data = customer_data.withColumn(\"cart_value\", (floor(rand() * 10) + 1) * 500)\n",
    "\n",
    "# Add a new column Age\n",
    "customer_data = customer_data.withColumn(\"Age\", floor(rand() * 26) + 25)\n",
    "\n",
    "# Show the DataFrame with the updated columns\n",
    "customer_data.show(truncate=False)\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9ce5e",
   "metadata": {},
   "source": [
    "# Displaying the data of customer having age>=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50e0b0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "|cust_id|cust_fname|cust_lname|cust_order_quantity|payment_status|cust_fullName |cart_value|Age|\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "|1      |Rahul     |Kumar     |5                  |paid          |Rahul Kumar   |5000      |34 |\n",
      "|2      |Priya     |Sharma    |8                  |paid          |Priya Sharma  |4000      |43 |\n",
      "|4      |Sneha     |Verma     |6                  |paid          |Sneha Verma   |2500      |31 |\n",
      "|5      |Amit      |Joshi     |10                 |unpaid        |Amit Joshi    |2000      |47 |\n",
      "|6      |Neha      |Patel     |4                  |paid          |Neha Patel    |5000      |35 |\n",
      "|8      |Deepak    |Reddy     |2                  |paid          |Deepak Reddy  |4500      |32 |\n",
      "|9      |Pooja     |Chopra    |9                  |paid          |Pooja Chopra  |2000      |44 |\n",
      "|11     |Aarti     |Verma     |7                  |paid          |Aarti Verma   |3500      |35 |\n",
      "|12     |Karan     |Mehta     |3                  |unpaid        |Karan Mehta   |500       |49 |\n",
      "|13     |Anjali    |Agarwal   |6                  |paid          |Anjali Agarwal|5000      |44 |\n",
      "|15     |Shweta    |Gupta     |4                  |paid          |Shweta Gupta  |2500      |42 |\n",
      "|16     |Rohit     |Kapoor    |10                 |unpaid        |Rohit Kapoor  |1500      |40 |\n",
      "|18     |Ajay      |Kumar     |7                  |unpaid        |Ajay Kumar    |1000      |45 |\n",
      "|19     |Mona      |Singh     |2                  |paid          |Mona Singh    |5000      |33 |\n",
      "|20     |Arjun     |Sharma    |9                  |paid          |Arjun Sharma  |3500      |35 |\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat_ws, rand, floor\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CustomerData\").getOrCreate()\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_file_path = \"C:/Users/vaibhav/Desktop/jupyter projects/customerData.csv\"\n",
    "\n",
    "# Read CSV into a Spark DataFrame\n",
    "customer_data = spark.read.csv(csv_file_path, header=True)\n",
    "\n",
    "# Concatenate cust_fname and cust_lname columns\n",
    "customer_data = customer_data.withColumn(\"cust_fullName\", concat_ws(\" \", \"cust_fname\", \"cust_lname\"))\n",
    "\n",
    "# Add a new column cart_value \n",
    "customer_data = customer_data.withColumn(\"cart_value\", (floor(rand() * 10) + 1) * 500)\n",
    "\n",
    "# Add a new column Age with random values between 25 and 50\n",
    "customer_data = customer_data.withColumn(\"Age\", floor(rand() * 26) + 25)\n",
    "\n",
    "# Displays the data of customers with age >= 30\n",
    "filtered_data = customer_data.filter(col(\"Age\") >= 30)\n",
    "filtered_data.show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2941c4",
   "metadata": {},
   "source": [
    "# Arranging the data according tp the age of customers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdc3d7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "|cust_id|cust_fname|cust_lname|cust_order_quantity|payment_status|cust_fullName |cart_value|Age|\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "|1      |Rahul     |Kumar     |5                  |paid          |Rahul Kumar   |1500      |32 |\n",
      "|2      |Priya     |Sharma    |8                  |paid          |Priya Sharma  |3000      |50 |\n",
      "|3      |Ankit     |Singh     |3                  |unpaid        |Ankit Singh   |4500      |33 |\n",
      "|4      |Sneha     |Verma     |6                  |paid          |Sneha Verma   |4000      |29 |\n",
      "|5      |Amit      |Joshi     |10                 |unpaid        |Amit Joshi    |3000      |45 |\n",
      "|6      |Neha      |Patel     |4                  |paid          |Neha Patel    |2500      |34 |\n",
      "|7      |Raj       |Shah      |7                  |unpaid        |Raj Shah      |1500      |26 |\n",
      "|8      |Deepak    |Reddy     |2                  |paid          |Deepak Reddy  |5000      |45 |\n",
      "|9      |Pooja     |Chopra    |9                  |paid          |Pooja Chopra  |4500      |37 |\n",
      "|10     |Suresh    |Mishra    |5                  |unpaid        |Suresh Mishra |2500      |47 |\n",
      "|11     |Aarti     |Verma     |7                  |paid          |Aarti Verma   |3000      |35 |\n",
      "|12     |Karan     |Mehta     |3                  |unpaid        |Karan Mehta   |4500      |33 |\n",
      "|13     |Anjali    |Agarwal   |6                  |paid          |Anjali Agarwal|5000      |47 |\n",
      "|14     |Vikram    |Singh     |8                  |unpaid        |Vikram Singh  |2000      |28 |\n",
      "|15     |Shweta    |Gupta     |4                  |paid          |Shweta Gupta  |2500      |29 |\n",
      "|16     |Rohit     |Kapoor    |10                 |unpaid        |Rohit Kapoor  |3500      |29 |\n",
      "|17     |Divya     |Mittal    |5                  |paid          |Divya Mittal  |3500      |27 |\n",
      "|18     |Ajay      |Kumar     |7                  |unpaid        |Ajay Kumar    |500       |41 |\n",
      "|19     |Mona      |Singh     |2                  |paid          |Mona Singh    |4500      |48 |\n",
      "|20     |Arjun     |Sharma    |9                  |paid          |Arjun Sharma  |500       |30 |\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "\n",
      "\n",
      "Sorted Data (Ascending Order based on Age):\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "|cust_id|cust_fname|cust_lname|cust_order_quantity|payment_status|cust_fullName |cart_value|Age|\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "|7      |Raj       |Shah      |7                  |unpaid        |Raj Shah      |1500      |26 |\n",
      "|17     |Divya     |Mittal    |5                  |paid          |Divya Mittal  |3500      |27 |\n",
      "|14     |Vikram    |Singh     |8                  |unpaid        |Vikram Singh  |2000      |28 |\n",
      "|4      |Sneha     |Verma     |6                  |paid          |Sneha Verma   |4000      |29 |\n",
      "|15     |Shweta    |Gupta     |4                  |paid          |Shweta Gupta  |2500      |29 |\n",
      "|16     |Rohit     |Kapoor    |10                 |unpaid        |Rohit Kapoor  |3500      |29 |\n",
      "|20     |Arjun     |Sharma    |9                  |paid          |Arjun Sharma  |500       |30 |\n",
      "|1      |Rahul     |Kumar     |5                  |paid          |Rahul Kumar   |1500      |32 |\n",
      "|3      |Ankit     |Singh     |3                  |unpaid        |Ankit Singh   |4500      |33 |\n",
      "|12     |Karan     |Mehta     |3                  |unpaid        |Karan Mehta   |4500      |33 |\n",
      "|6      |Neha      |Patel     |4                  |paid          |Neha Patel    |2500      |34 |\n",
      "|11     |Aarti     |Verma     |7                  |paid          |Aarti Verma   |3000      |35 |\n",
      "|9      |Pooja     |Chopra    |9                  |paid          |Pooja Chopra  |4500      |37 |\n",
      "|18     |Ajay      |Kumar     |7                  |unpaid        |Ajay Kumar    |500       |41 |\n",
      "|5      |Amit      |Joshi     |10                 |unpaid        |Amit Joshi    |3000      |45 |\n",
      "|8      |Deepak    |Reddy     |2                  |paid          |Deepak Reddy  |5000      |45 |\n",
      "|10     |Suresh    |Mishra    |5                  |unpaid        |Suresh Mishra |2500      |47 |\n",
      "|13     |Anjali    |Agarwal   |6                  |paid          |Anjali Agarwal|5000      |47 |\n",
      "|19     |Mona      |Singh     |2                  |paid          |Mona Singh    |4500      |48 |\n",
      "|2      |Priya     |Sharma    |8                  |paid          |Priya Sharma  |3000      |50 |\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat_ws, rand, floor, col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CustomerData\").getOrCreate()\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_file_path = \"C:/Users/vaibhav/Desktop/jupyter projects/customerData.csv\"\n",
    "\n",
    "# Read CSV into a Spark DataFrame\n",
    "customer_data = spark.read.csv(csv_file_path, header=True)\n",
    "\n",
    "# Concatenate cust_fname and cust_lname columns\n",
    "customer_data = customer_data.withColumn(\"cust_fullName\", concat_ws(\" \", \"cust_fname\", \"cust_lname\"))\n",
    "\n",
    "# Add a new column cart_value\n",
    "customer_data = customer_data.withColumn(\"cart_value\", (floor(rand() * 10) + 1) * 500)\n",
    "\n",
    "# Add a new column Age\n",
    "customer_data = customer_data.withColumn(\"Age\", floor(rand() * 26) + 25)\n",
    "\n",
    "# Display the original data\n",
    "print(\"Original Data:\")\n",
    "customer_data.show(truncate=False)\n",
    "\n",
    "# Arrange the data in ascending order based on the \"Age\" column\n",
    "sorted_data = customer_data.orderBy(\"Age\")\n",
    "\n",
    "# Display the sorted data\n",
    "print(\"\\nSorted Data (Ascending Order based on Age):\")\n",
    "sorted_data.show(truncate=False)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4d808",
   "metadata": {},
   "source": [
    "# EXPORTING THE DATA OF customerData.csv TO updatedCustData.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd596172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "|cust_id|cust_fname|cust_lname|cust_order_quantity|payment_status|cust_fullName |cart_value|Age|\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "|2      |Priya     |Sharma    |8                  |paid          |Priya Sharma  |3000      |47 |\n",
      "|3      |Ankit     |Singh     |3                  |unpaid        |Ankit Singh   |2000      |42 |\n",
      "|6      |Neha      |Patel     |4                  |paid          |Neha Patel    |1500      |35 |\n",
      "|7      |Raj       |Shah      |7                  |unpaid        |Raj Shah      |1000      |31 |\n",
      "|8      |Deepak    |Reddy     |2                  |paid          |Deepak Reddy  |3500      |43 |\n",
      "|9      |Pooja     |Chopra    |9                  |paid          |Pooja Chopra  |500       |34 |\n",
      "|11     |Aarti     |Verma     |7                  |paid          |Aarti Verma   |3500      |39 |\n",
      "|13     |Anjali    |Agarwal   |6                  |paid          |Anjali Agarwal|3500      |33 |\n",
      "|14     |Vikram    |Singh     |8                  |unpaid        |Vikram Singh  |1000      |42 |\n",
      "|15     |Shweta    |Gupta     |4                  |paid          |Shweta Gupta  |3500      |39 |\n",
      "|16     |Rohit     |Kapoor    |10                 |unpaid        |Rohit Kapoor  |3000      |35 |\n",
      "|17     |Divya     |Mittal    |5                  |paid          |Divya Mittal  |4000      |46 |\n",
      "|18     |Ajay      |Kumar     |7                  |unpaid        |Ajay Kumar    |4000      |43 |\n",
      "|20     |Arjun     |Sharma    |9                  |paid          |Arjun Sharma  |2000      |43 |\n",
      "+-------+----------+----------+-------------------+--------------+--------------+----------+---+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o380.csv.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m filtered_data\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m output_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/vaibhav/Desktop/jupyter projects/updatedCustData.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mfiltered_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o380.csv.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat_ws, rand, floor, col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CustomerData\").getOrCreate()\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_file_path = \"C:/Users/vaibhav/Desktop/jupyter projects/customerData.csv\"\n",
    "\n",
    "# Read CSV into a Spark DataFrame\n",
    "customer_data = spark.read.csv(csv_file_path, header=True)\n",
    "\n",
    "# Concatenate cust_fname and cust_lname columns\n",
    "customer_data = customer_data.withColumn(\"cust_fullName\", concat_ws(\" \", \"cust_fname\", \"cust_lname\"))\n",
    "\n",
    "# Add a new column cart_valu\n",
    "customer_data = customer_data.withColumn(\"cart_value\", (floor(rand() * 10) + 1) * 500)\n",
    "\n",
    "# Add a new column Age\n",
    "customer_data = customer_data.withColumn(\"Age\", floor(rand() * 26) + 25)\n",
    "\n",
    "# Display the data of customers with age >= 30\n",
    "filtered_data = customer_data.filter(col(\"Age\") >= 30)\n",
    "filtered_data.show(truncate=False)\n",
    "\n",
    "output_csv_path = \"C:/Users/vaibhav/Desktop/jupyter projects/updatedCustData.csv\"\n",
    "\n",
    "filtered_data.write.csv(output_csv_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c5d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
